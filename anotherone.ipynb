{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, optimizers\n",
    "import cv2 as cv\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_pre, y_train_pre) , (x_test_pre,y_test_pre) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "x_full = np.concatenate([x_train_pre, x_test_pre], axis=0)\n",
    "y_full = np.concatenate([y_train_pre, y_test_pre], axis=0)\n",
    "\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_full, y_full, test_size=0.30,random_state=41)\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.50,random_state=35)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "things = ['airplane','automobile','bird','cat','deer','dog','frog',\n",
    "          'horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds the layers that are processed in order for data augmentaton to occur \n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    #tf.keras.layers.RandomCrop(32, 32),\n",
    "    layers.Rescaling(1./255), \n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.1),\n",
    "\n",
    "])\n",
    "\n",
    "#sets the build for the augmentation layer so it fits with the model \n",
    "#properly \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x,filters,reg,dropout_rate,downsample=False, expansion=4):\n",
    "\n",
    "    #what this does is save the input value which is kinda like the important thing abt residual\n",
    "    shortcut_x = x \n",
    "    strides = 2 if downsample else 1\n",
    "\n",
    "    x = layers.Conv2D(filters, (1,1), padding='same', strides=1,\n",
    "                      kernel_initializer='he_normal', kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    x = layers.Conv2D(filters, (3,3), padding='same', strides=strides,\n",
    "                      kernel_initializer='he_normal', kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    x = layers.Conv2D(filters * expansion, (1,1), padding='same', strides=1,\n",
    "                      kernel_initializer='he_normal', kernel_regularizer=reg)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if downsample or shortcut_x.shape[-1] != filters * expansion:\n",
    "        shortcut_x = layers.Conv2D(filters * expansion, (1,1), padding='same', strides=strides,\n",
    "                                   kernel_initializer='he_normal', kernel_regularizer=reg)(shortcut_x)\n",
    "        shortcut_x = layers.BatchNormalization()(shortcut_x)\n",
    "        \n",
    "    x = layers.Add()([x, shortcut_x])\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regularizer = regularizers.L2(1e-4)\n",
    "\n",
    "inputs = keras.Input(shape=(32,32,3))\n",
    "\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "x = layers.Conv2D(64, kernel_size=(7,7), strides=2, kernel_initializer='he_normal', \n",
    "                  kernel_regularizer=regularizer, padding='same')(x)    \n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "x = residual_block(x, 64, regularizer, dropout_rate=0.1, downsample=True)\n",
    "x = residual_block(x, 64, regularizer, dropout_rate=0.1)\n",
    "\n",
    "x = residual_block(x, 128, regularizer, dropout_rate=0.15, downsample=True)\n",
    "x = residual_block(x, 128, regularizer, dropout_rate=0.15)\n",
    "\n",
    "x = residual_block(x, 256, regularizer, dropout_rate=0.2, downsample=True)\n",
    "x = residual_block(x, 256, regularizer, dropout_rate=0.2)\n",
    "\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "x= layers.Dense(256, activation='relu', kernel_initializer='he_normal',kernel_regularizer=regularizer)(x)\n",
    "x= layers.BatchNormalization()(x)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "output= layers.Dense(10, activation='softmax',\n",
    "                          kernel_initializer='he_normal',kernel_regularizer=regularizer)(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "resmodel = keras.Model(inputs,output)\n",
    "\n",
    "decay_steps = 657 * 25\n",
    "initial_learning_rate = 0.0008\n",
    "alpha = .001\n",
    "lr_decayed_fn = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate, decay_steps,alpha)\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(lr_decayed_fn)\n",
    "\n",
    "resmodel.compile(optimizer=optimizer, \n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
    "    metrics=['accuracy'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.arange(0, decay_steps)\n",
    "lrs = [lr_decayed_fn(step) for step in steps]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, lrs)\n",
    "plt.title('Cosine Decay Learning Rate Schedule')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.yscale('log')\n",
    "plt.vlines(x=25*657,ymin=0,ymax=10e-3)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = resmodel.fit(       \n",
    "    x_train, y_train, batch_size=64,\n",
    "    validation_data=(x_val,y_val),\n",
    "    epochs=25,\n",
    "    callbacks=[\n",
    "    keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,  \n",
    "    restore_best_weights=True,\n",
    "    verbose=1),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resmodel.fit(x_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
